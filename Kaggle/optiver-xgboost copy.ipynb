{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 5.356036,
     "end_time": "2023-12-06T20:12:49.248745",
     "exception": false,
     "start_time": "2023-12-06T20:12:43.892709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc  \n",
    "import os  \n",
    "import time  \n",
    "import warnings \n",
    "from itertools import combinations  \n",
    "from warnings import simplefilter \n",
    "import joblib  \n",
    "import lightgbm as lgb  \n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "from sklearn.metrics import mean_absolute_error,mean_absolute_percentage_error\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit  \n",
    "import polars as pl\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "is_offline = False \n",
    "LGB = True\n",
    "NN = False\n",
    "is_train = True  \n",
    "is_infer = True \n",
    "max_lookback = np.nan \n",
    "split_day = 435  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "papermill": {
     "duration": 0.016612,
     "end_time": "2023-12-06T20:12:49.308828",
     "exception": false,
     "start_time": "2023-12-06T20:12:49.292216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weighted_average(a):\n",
    "    w = []\n",
    "    n = len(a)\n",
    "    for j in range(1, n + 1):\n",
    "        j = 2 if j == 1 else j\n",
    "        w.append(1 / (2**(n + 1 - j)))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 0.03023,
     "end_time": "2023-12-06T20:12:49.347647",
     "exception": false,
     "start_time": "2023-12-06T20:12:49.317417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                \n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "            \n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "                    \n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "papermill": {
     "duration": 0.023317,
     "end_time": "2023-12-06T20:12:49.379333",
     "exception": false,
     "start_time": "2023-12-06T20:12:49.356016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=0):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "               \n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "    if verbose:\n",
    "        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
    "        decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "        logger.info(f\"Decreased by {decrease:.2f}%\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 18.350404,
     "end_time": "2023-12-06T20:13:07.754692",
     "exception": false,
     "start_time": "2023-12-06T20:12:49.404288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df = df.dropna(subset=[\"target\"])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df_shape = df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "papermill": {
     "duration": 0.64821,
     "end_time": "2023-12-06T20:13:08.473234",
     "exception": false,
     "start_time": "2023-12-06T20:13:07.825024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numba import njit, prange\n",
    "\n",
    "@njit(parallel=True)\n",
    "def compute_triplet_imbalance(df_values, comb_indices):\n",
    "    num_rows = df_values.shape[0]\n",
    "    num_combinations = len(comb_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "    for i in prange(num_combinations):\n",
    "        a, b, c = comb_indices[i]\n",
    "        for j in range(num_rows):\n",
    "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
    "            \n",
    "            if mid_val == min_val:\n",
    "                imbalance_features[j, i] = np.nan\n",
    "            else:\n",
    "                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n",
    "\n",
    "    return imbalance_features\n",
    "\n",
    "def calculate_triplet_imbalance_numba(price, df):\n",
    "    df_values = df[price].values\n",
    "    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n",
    "    features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
    "    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imbalance_features(df):\n",
    "    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "    flag = ['imbalance_buy_sell_flag']\n",
    "\n",
    "\n",
    "    # Size Imbalances \n",
    "    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n",
    "    df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n",
    "    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n",
    "    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n",
    "    df['imbalance_incline']  =  df.eval(\"(imbalance_buy_sell_flag * imbalance_size)/(matched_size+imbalance_size)\") # imbalance direction\n",
    "    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Prices Imbalances\n",
    "    for c in combinations(prices, 2):\n",
    "        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n",
    "\n",
    "    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n",
    "        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n",
    "        df[triplet_feature.columns] = triplet_feature.values\n",
    "    \n",
    "    df[\"stock_weights\"] = df[\"stock_id\"].map(weights)\n",
    "    df[\"weighted_wap\"] = df[\"stock_weights\"] * df[\"wap\"]\n",
    "    df['wap_momentum'] = df.groupby('stock_id')['weighted_wap'].pct_change(periods=6)\n",
    "\n",
    "\n",
    "    # Hypothetical  near price and far price upper/lower limit \n",
    "    df['near_price_lower_limit'] = df.eval('near_price * (0.95)') \n",
    "    df['near_price_lower_limit'] = df.eval('near_price * (1.05)') \n",
    "    \n",
    "    df['far_price_lower_limit'] = df.eval('far_price * (.95)') \n",
    "    df['far_price_upper_limit'] = df.eval('far_price * (1.05)') \n",
    "\n",
    "\n",
    "    \n",
    "    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n",
    "    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n",
    "    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n",
    "    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n",
    "    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n",
    "    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n",
    "    \n",
    "    df['spread_depth_ratio'] = (df['ask_price'] - df['bid_price']) / (df['bid_size'] + df['ask_size'])\n",
    "    df['mid_price_movement'] = df['mid_price'].diff(periods=5).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
    "    \n",
    "    df['micro_price'] = ((df['bid_price'] * df['ask_size']) + (df['ask_price'] * df['bid_size'])) / (df['bid_size'] + df['ask_size'])\n",
    "    df['relative_spread'] = (df['ask_price'] - df['bid_price']) / df['wap']\n",
    "    \n",
    "    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n",
    "        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n",
    "        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n",
    "        \n",
    "\n",
    "    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n",
    "        for window in [1,2,3,5,7]:\n",
    "            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n",
    "            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n",
    "    \n",
    "    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size', 'weighted_wap','price_spread']:\n",
    "        for window in [1,2,3,5,7]:\n",
    "            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n",
    "    \n",
    "    for window in [3,5,7]:\n",
    "        df[f'price_change_diff_{window}'] = df[f'bid_price_diff_{window}'] - df[f'ask_price_diff_{window}']\n",
    "        df[f'size_change_diff_{window}'] = df[f'bid_size_diff_{window}'] - df[f'ask_size_diff_{window}']\n",
    "\n",
    "    pl_df = pl.from_pandas(df)\n",
    "\n",
    "    windows = [3, 5, 7]\n",
    "    columns = ['ask_price', 'bid_price', 'ask_size', 'bid_size']\n",
    "\n",
    "    group = [\"stock_id\"]\n",
    "    expressions = []\n",
    "\n",
    "    for window in windows:\n",
    "        for col in columns:\n",
    "            rolling_mean_expr = (\n",
    "                pl.col(f\"{col}_diff_{window}\")\n",
    "                .rolling_mean(window)\n",
    "                .over(group)\n",
    "                .alias(f'rolling_diff_{col}_{window}')\n",
    "            )\n",
    "\n",
    "            rolling_std_expr = (\n",
    "                pl.col(f\"{col}_diff_{window}\")\n",
    "                .rolling_std(window)\n",
    "                .over(group)\n",
    "                .alias(f'rolling_std_diff_{col}_{window}')\n",
    "            )\n",
    "\n",
    "            expressions.append(rolling_mean_expr)\n",
    "            expressions.append(rolling_std_expr)\n",
    "\n",
    "    lazy_df = pl_df.lazy().with_columns(expressions)\n",
    "\n",
    "    pl_df = lazy_df.collect()\n",
    "\n",
    "    df = pl_df.to_pandas()\n",
    "    gc.collect()\n",
    "    \n",
    "    df['mid_price*volume'] = df['mid_price_movement'] * df['volume']\n",
    "    df['harmonic_imbalance'] = df.eval('2 / ((1 / bid_size) + (1 / ask_size))')\n",
    "    \n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Temporal Fetaures\n",
    "def other_features(df):\n",
    "    df[\"dow\"] = df[\"date_id\"] % 5  # Day of the week\n",
    "    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  \n",
    "    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60  \n",
    "    df['time_to_market_close'] = 540 - df['seconds_in_bucket']\n",
    "    \n",
    "    for key, value in global_stock_id_feats.items():\n",
    "        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n",
    "\n",
    "    return df\n",
    "\n",
    "def generate_all_features(df):\n",
    "    cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\", \"target\"]]\n",
    "    df = df[cols]\n",
    "    \n",
    "    df = imbalance_features(df)\n",
    "    gc.collect() \n",
    "    df = other_features(df)\n",
    "    gc.collect()  \n",
    "    feature_name = [i for i in df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]\n",
    "    \n",
    "    return df[feature_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "papermill": {
     "duration": 0.023916,
     "end_time": "2023-12-06T20:13:08.570062",
     "exception": false,
     "start_time": "2023-12-06T20:13:08.546146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = [\n",
    "    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n",
    "    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n",
    "    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n",
    "    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n",
    "    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n",
    "    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n",
    "    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n",
    "    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n",
    "    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n",
    "    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n",
    "    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n",
    "    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n",
    "    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n",
    "    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n",
    "    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n",
    "    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n",
    "    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n",
    "]\n",
    "weights = {int(k):v for k,v in enumerate(weights)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "papermill": {
     "duration": 0.01642,
     "end_time": "2023-12-06T20:13:08.612117",
     "exception": false,
     "start_time": "2023-12-06T20:13:08.595697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Online mode\n"
     ]
    }
   ],
   "source": [
    "if is_offline:\n",
    "    \n",
    "    df_train = df[df[\"date_id\"] <= split_day]\n",
    "    df_valid = df[df[\"date_id\"] > split_day]\n",
    "    print(\"Offline mode\")\n",
    "    print(f\"train : {df_train.shape}, valid : {df_valid.shape}\")\n",
    "    \n",
    "else:\n",
    "    df_train = df\n",
    "    print(\"Online mode\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "papermill": {
     "duration": 89.549827,
     "end_time": "2023-12-06T20:14:38.170382",
     "exception": false,
     "start_time": "2023-12-06T20:13:08.620555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build Online Train Feats Finished.\n"
     ]
    }
   ],
   "source": [
    "if is_train:\n",
    "    global_stock_id_feats = {\n",
    "        \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n",
    "        \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
    "        \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
    "        \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n",
    "        \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
    "        \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
    "    }\n",
    "    if is_offline:\n",
    "        df_train_feats = generate_all_features(df_train)\n",
    "        print(\"Build Train Feats Finished.\")\n",
    "        df_valid_feats = generate_all_features(df_valid)\n",
    "        print(\"Build Valid Feats Finished.\")\n",
    "        df_valid_feats = reduce_mem_usage(df_valid_feats)\n",
    "    else:\n",
    "        df_train_feats = generate_all_features(df_train)\n",
    "        print(\"Build Online Train Feats Finished.\")\n",
    "\n",
    "    df_train_feats = reduce_mem_usage(df_train_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_importance_idx = np.array([\n",
    "    0,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
    "         14,  16,  17,  19,  20,  24,  25,  26,  27,  28,  29,  30,  31,\n",
    "         32,  33,  35,  37,  38,  39,  40,  41,  42,  43,  44,  45,  46,\n",
    "         47,  48,  49,  50,  51,  53,  54,  55,  56,  57,  58,  59,  60,\n",
    "         61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,\n",
    "         74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,  85,  86,\n",
    "         87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99,\n",
    "        100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n",
    "        113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
    "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138,\n",
    "        139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151,\n",
    "        152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164,\n",
    "        165, 166, 167, 168, 169, 170, 171\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_feats.drop(df_train_feats.columns[not_importance_idx], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seconds_in_bucket</th>\n",
       "      <th>liquidity_imbalance</th>\n",
       "      <th>size_imbalance</th>\n",
       "      <th>reference_price_ask_price_imb</th>\n",
       "      <th>reference_price_bid_price_imb</th>\n",
       "      <th>reference_price_wap_imb</th>\n",
       "      <th>ask_price_bid_price_wap_imb2</th>\n",
       "      <th>ask_price_wap_reference_price_imb2</th>\n",
       "      <th>market_urgency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.754340</td>\n",
       "      <td>7.141326</td>\n",
       "      <td>-0.000107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000094</td>\n",
       "      <td>0.138298</td>\n",
       "      <td>1.382979e-01</td>\n",
       "      <td>0.000161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.728751</td>\n",
       "      <td>0.156905</td>\n",
       "      <td>-0.000382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>6.346154</td>\n",
       "      <td>6.346154e+00</td>\n",
       "      <td>-0.000557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.332935</td>\n",
       "      <td>1.998210</td>\n",
       "      <td>-0.000369</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>-0.000220</td>\n",
       "      <td>0.499162</td>\n",
       "      <td>6.788155e-01</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.990340</td>\n",
       "      <td>0.004853</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>2.514620e-01</td>\n",
       "      <td>-0.000213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.948687</td>\n",
       "      <td>37.976364</td>\n",
       "      <td>-0.000242</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>-0.000234</td>\n",
       "      <td>0.026403</td>\n",
       "      <td>3.418804e-02</td>\n",
       "      <td>0.000590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237887</th>\n",
       "      <td>540</td>\n",
       "      <td>-0.816784</td>\n",
       "      <td>0.100847</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>9.636364</td>\n",
       "      <td>9.636364e+00</td>\n",
       "      <td>-0.000096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237888</th>\n",
       "      <td>540</td>\n",
       "      <td>0.374254</td>\n",
       "      <td>2.196184</td>\n",
       "      <td>-0.000128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000088</td>\n",
       "      <td>0.460227</td>\n",
       "      <td>4.602273e-01</td>\n",
       "      <td>0.000096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237889</th>\n",
       "      <td>540</td>\n",
       "      <td>-0.829388</td>\n",
       "      <td>0.093262</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>10.750000</td>\n",
       "      <td>1.075000e+01</td>\n",
       "      <td>-0.000078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237890</th>\n",
       "      <td>540</td>\n",
       "      <td>-0.684154</td>\n",
       "      <td>0.187540</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>5.315790</td>\n",
       "      <td>-1.099231e-12</td>\n",
       "      <td>-0.000164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237891</th>\n",
       "      <td>540</td>\n",
       "      <td>-0.091024</td>\n",
       "      <td>0.833139</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>1.193103</td>\n",
       "      <td>1.193103e+00</td>\n",
       "      <td>-0.000029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5237892 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         seconds_in_bucket  liquidity_imbalance  size_imbalance  \\\n",
       "0                        0             0.754340        7.141326   \n",
       "1                        0            -0.728751        0.156905   \n",
       "2                        0             0.332935        1.998210   \n",
       "3                        0            -0.990340        0.004853   \n",
       "4                        0             0.948687       37.976364   \n",
       "...                    ...                  ...             ...   \n",
       "5237887                540            -0.816784        0.100847   \n",
       "5237888                540             0.374254        2.196184   \n",
       "5237889                540            -0.829388        0.093262   \n",
       "5237890                540            -0.684154        0.187540   \n",
       "5237891                540            -0.091024        0.833139   \n",
       "\n",
       "         reference_price_ask_price_imb  reference_price_bid_price_imb  \\\n",
       "0                            -0.000107                       0.000000   \n",
       "1                            -0.000382                       0.000000   \n",
       "2                            -0.000369                       0.000079   \n",
       "3                            -0.000021                       0.000086   \n",
       "4                            -0.000242                       0.000069   \n",
       "...                                ...                            ...   \n",
       "5237887                      -0.000058                       0.000000   \n",
       "5237888                      -0.000128                       0.000000   \n",
       "5237889                      -0.000047                       0.000000   \n",
       "5237890                       0.000000                       0.000120   \n",
       "5237891                      -0.000159                       0.000000   \n",
       "\n",
       "         reference_price_wap_imb  ask_price_bid_price_wap_imb2  \\\n",
       "0                      -0.000094                      0.138298   \n",
       "1                      -0.000052                      6.346154   \n",
       "2                      -0.000220                      0.499162   \n",
       "3                       0.000085                    214.000000   \n",
       "4                      -0.000234                      0.026403   \n",
       "...                          ...                           ...   \n",
       "5237887                -0.000005                      9.636364   \n",
       "5237888                -0.000088                      0.460227   \n",
       "5237889                -0.000004                     10.750000   \n",
       "5237890                 0.000101                      5.315790   \n",
       "5237891                -0.000072                      1.193103   \n",
       "\n",
       "         ask_price_wap_reference_price_imb2  market_urgency  \n",
       "0                              1.382979e-01        0.000161  \n",
       "1                              6.346154e+00       -0.000557  \n",
       "2                              6.788155e-01        0.000298  \n",
       "3                              2.514620e-01       -0.000213  \n",
       "4                              3.418804e-02        0.000590  \n",
       "...                                     ...             ...  \n",
       "5237887                        9.636364e+00       -0.000096  \n",
       "5237888                        4.602273e-01        0.000096  \n",
       "5237889                        1.075000e+01       -0.000078  \n",
       "5237890                       -1.099231e-12       -0.000164  \n",
       "5237891                        1.193103e+00       -0.000029  \n",
       "\n",
       "[5237892 rows x 9 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "papermill": {
     "duration": 10840.462444,
     "end_time": "2023-12-06T23:15:18.765107",
     "exception": false,
     "start_time": "2023-12-06T20:14:38.302663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features = 9\n",
      "Fold 1 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's mape: 0.943022\n",
      "Model for fold 1 saved to models_params_save/model_1.txt\n",
      ":LGB Fold 1 MAPE: 567.0961918388251\n",
      "Fold 2 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's mape: 0.937597\n",
      "Model for fold 2 saved to models_params_save/model_2.txt\n",
      ":LGB Fold 2 MAPE: 657.8597542678232\n",
      "Fold 3 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's mape: 0.935931\n",
      "Model for fold 3 saved to models_params_save/model_3.txt\n",
      ":LGB Fold 3 MAPE: 756.7635444803085\n",
      "Fold 4 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's mape: 0.930841\n",
      "Model for fold 4 saved to models_params_save/model_4.txt\n",
      ":LGB Fold 4 MAPE: 453.37250822900114\n",
      "Fold 5 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's mape: 0.914396\n",
      "Model for fold 5 saved to models_params_save/model_5.txt\n",
      ":LGB Fold 5 MAPE: 380.66629972877337\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if LGB:\n",
    "    import numpy as np\n",
    "    import lightgbm as lgb\n",
    "    \n",
    "    lgb_params = {\n",
    "        \"objective\": \"mape\",\n",
    "        \"n_estimators\": 20,\n",
    "        # \"num_leaves\": 128,\n",
    "        \"subsample\": 0.6,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"learning_rate\": 0.05,\n",
    "        'max_depth': 11,\n",
    "        \"n_jobs\": 4,\n",
    "        \"verbosity\": -1,\n",
    "        \"importance_type\": \"gain\",\n",
    "        \"reg_alpha\": 0.2,\n",
    "        \"reg_lambda\": 3,\n",
    "        'device':'cpu'\n",
    "    }\n",
    "\n",
    "    feature_columns = list(df_train_feats.columns)\n",
    "    print(f\"Features = {len(feature_columns)}\")\n",
    "\n",
    "    num_folds = 5\n",
    "    fold_size = 480 // num_folds\n",
    "    gap = 5\n",
    "\n",
    "    models = []\n",
    "    models_cbt = []\n",
    "    scores = []\n",
    "\n",
    "    model_save_path = 'models_params_save' \n",
    "    if not os.path.exists(model_save_path):\n",
    "        os.makedirs(model_save_path)\n",
    "\n",
    "    date_ids = df_train['date_id'].values\n",
    "\n",
    "    for i in range(num_folds):\n",
    "        start = i * fold_size\n",
    "        end = start + fold_size\n",
    "        if i < num_folds - 1:  \n",
    "            purged_start = end - 2\n",
    "            purged_end = end + gap + 2\n",
    "            train_indices = (date_ids >= start) & (date_ids < purged_start) | (date_ids > purged_end)\n",
    "        else:\n",
    "            train_indices = (date_ids >= start) & (date_ids < end)\n",
    "\n",
    "        test_indices = (date_ids >= end) & (date_ids < end + fold_size)\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "        df_fold_train = df_train_feats[train_indices]\n",
    "        df_fold_train_target = df_train['target'][train_indices]\n",
    "        df_fold_valid = df_train_feats[test_indices]\n",
    "        df_fold_valid_target = df_train['target'][test_indices]\n",
    "\n",
    "        print(f\"Fold {i+1} Model Training\")\n",
    "\n",
    "        lgb_model = lgb.LGBMRegressor(**lgb_params)\n",
    "        lgb_model.fit(\n",
    "            df_fold_train[feature_columns],\n",
    "            df_fold_train_target,\n",
    "            eval_set=[(df_fold_valid[feature_columns], df_fold_valid_target)],\n",
    "            callbacks=[\n",
    "                lgb.callback.early_stopping(stopping_rounds=100),\n",
    "                lgb.callback.log_evaluation(period=100),\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "        models.append(lgb_model)\n",
    "        model_filename = os.path.join(model_save_path, f'model_{i+1}.txt')\n",
    "        lgb_model.booster_.save_model(model_filename)\n",
    "        print(f\"Model for fold {i+1} saved to {model_filename}\")\n",
    "\n",
    "\n",
    "        fold_predictions = lgb_model.predict(df_fold_valid[feature_columns])\n",
    "        fold_score = mean_absolute_percentage_error(fold_predictions, df_fold_valid_target)\n",
    "        scores.append(fold_score)\n",
    "        print(f\":LGB Fold {i+1} MAPE: {fold_score}\")\n",
    "\n",
    "        del df_fold_train, df_fold_train_target, df_fold_valid, df_fold_valid_target\n",
    "        gc.collect()\n",
    "\n",
    "    average_best_iteration = int(np.mean([model.best_iteration_ for model in models]))\n",
    "\n",
    "    final_model_params = lgb_params.copy()\n",
    "\n",
    "\n",
    "    num_model = 1\n",
    "\n",
    "    for i in range(num_model):\n",
    "        final_model = lgb.LGBMRegressor(**final_model_params)\n",
    "        final_model.fit(\n",
    "            df_train_feats[feature_columns],\n",
    "            df_train['target'],\n",
    "            callbacks=[\n",
    "                lgb.callback.log_evaluation(period=100),\n",
    "            ],\n",
    "        )\n",
    "        models.append(final_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = models[0].feature_importances_\n",
    "for i in range(1, len(models)): \n",
    "    weights += models[i].feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6887,  4847,  4650,  3864,  4349,  8012,  9772,  2315, 57322])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(weights).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64), 0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(weights<1000)[0], len(np.where(weights<1000)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['seconds_in_bucket', 'liquidity_imbalance', 'size_imbalance',\n",
       "       'reference_price_ask_price_imb', 'reference_price_bid_price_imb',\n",
       "       'reference_price_wap_imb', 'ask_price_bid_price_wap_imb2',\n",
       "       'ask_price_wap_reference_price_imb2', 'market_urgency'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_feats.columns[np.where(weights>100)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "papermill": {
     "duration": 238.738293,
     "end_time": "2023-12-06T23:19:17.94498",
     "exception": false,
     "start_time": "2023-12-06T23:15:19.206687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def zero_sum(prices, volumes):\n",
    "#     std_error = np.sqrt(volumes)\n",
    "#     step = np.sum(prices) / np.sum(std_error)\n",
    "#     out = prices - std_error * step\n",
    "#     return out\n",
    "\n",
    "# if is_infer:\n",
    "#     import optiver2023\n",
    "#     env = optiver2023.make_env()\n",
    "#     iter_test = env.iter_test()\n",
    "#     counter = 0\n",
    "#     y_min, y_max = -64, 64\n",
    "#     qps, predictions = [], []\n",
    "#     cache = pd.DataFrame()\n",
    "\n",
    "#     if LGB:\n",
    "#         lgb_model_weights = weighted_average(models)\n",
    "    \n",
    "#     for (test, revealed_targets, sample_prediction) in iter_test:\n",
    "#         now_time = time.time()\n",
    "#         cache = pd.concat([cache, test], ignore_index=True, axis=0)\n",
    "#         if counter > 0:\n",
    "#             cache = cache.groupby(['stock_id']).tail(21).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n",
    "#         feat = generate_all_features(cache)[-len(test):]\n",
    "#         print(f\"Feat Shape is: {feat.shape}\")\n",
    "\n",
    "#         if LGB:\n",
    "#             lgb_predictions = np.zeros(len(test))\n",
    "#             for model, weight in zip(models, lgb_model_weights):\n",
    "#                 lgb_predictions += weight * model.predict(feat[feature_columns])\n",
    "\n",
    "#         predictions = lgb_predictions\n",
    "        \n",
    "#         final_predictions = predictions - np.mean(predictions)\n",
    "#         clipped_predictions = np.clip(final_predictions, y_min, y_max)\n",
    "#         sample_prediction['target'] = clipped_predictions\n",
    "#         env.predict(sample_prediction)\n",
    "#         counter += 1\n",
    "#         qps.append(time.time() - now_time)\n",
    "#         if counter % 10 == 0:\n",
    "#             print(counter, 'qps:', np.mean(qps))\n",
    "\n",
    "#     time_cost = 1.146 * np.mean(qps)\n",
    "#     print(f\"The code will take approximately {np.round(time_cost, 4)} hours to reason about\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7056235,
     "sourceId": 57891,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30616,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11199.498291,
   "end_time": "2023-12-06T23:19:19.836264",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-06T20:12:40.337973",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
